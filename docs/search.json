[
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Blog and Note",
    "section": "",
    "text": "Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nDate\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\nJan 29, 2025\n\n\nToday’s Mood\n\n\nShu-Kai Hsieh\n\n\n\n\nOct 10, 2024\n\n\n在空中相會：從佛學到大型語言模型的跨學科對話\n\n\nShu-Kai Hsieh\n\n\n\n\nAug 15, 2024\n\n\n量子語言學筆記：波粒二象的啟示\n\n\nShu-Kai Hsieh\n\n\n\n\nAug 15, 2024\n\n\n量子語言學筆記：Lagrangian submanifold？\n\n\nShu-Kai Hsieh\n\n\n\n\nAug 15, 2024\n\n\n量子語言學筆記\n\n\nShu-Kai Hsieh\n\n\n\n\nOct 10, 2023\n\n\nNotes on ‘Modern language models refute Chomsky’s approach to language (Steven T. Piantadosi)’\n\n\nShu-Kai Hsieh\n\n\n\n\nMay 18, 2023\n\n\n布農語田調隨行記\n\n\n \n\n\n\n\nMar 5, 2023\n\n\n光\n\n\nShu-Kai Hsieh\n\n\n\n\nMar 5, 2023\n\n\nPrompting as a linguistic object\n\n\nShu-Kai Hsieh\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "謝舒凱 | Shu-Kai Hsieh",
    "section": "",
    "text": "Hi!\nI am Shu-Kai Hsieh [IPA: ɕjɛ].\nI am currently a joint-appointed professor at the Graduate Institute of Linguistics and the Brain and Mind Research Institute at National Taiwan University, Taiwan.\nI am interested in all things language, physics and consciousness, as well as the computational modeling of language in various aspects. I am leading a laboratory LOPE.\nAt my spare time, I enjoy reading Buddhist scriptures in different languages, playing the instrument, and petting dogs."
  },
  {
    "objectID": "posts/2023-04-23/index.html",
    "href": "posts/2023-04-23/index.html",
    "title": "Prompting as a linguistic object",
    "section": "",
    "text": "Prompting as a linguistic object\nPrompting LLMs has\n\n\n\n\nCitationBibTeX citation:@online{hsieh2023,\n  author = {Hsieh, Shu-Kai},\n  title = {Prompting as a Linguistic Object},\n  date = {2023-03-05},\n  url = {https://loperntu.github.io/posts/2023-04-23-quarto-blogs/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nHsieh, Shu-Kai. 2023. “Prompting as a Linguistic Object.”\nMarch 5, 2023. https://loperntu.github.io/posts/2023-04-23-quarto-blogs/."
  },
  {
    "objectID": "posts/2023-10-09/index.html",
    "href": "posts/2023-10-09/index.html",
    "title": "Notes on ‘Modern language models refute Chomsky’s approach to language (Steven T. Piantadosi)’",
    "section": "",
    "text": "Nature Machine Intelligence 在最近的主編的話 Language models and linguistic theories beyond words 再度把 LLM 和語言學的（沒有）關係拿到檯面上。雖然浮光掠影的談過，但是對於語言學知識較為友善的計算語言學家心裡的疑惑，是時候該拿出來討論了。\n簡單說，語言學在當前的AI語言模型發展進程中，基本上處在邊陲的角色。為什麼？是語言學（或主流的形式語言學範式）有什麼問題？我們又需要什麼樣的語言學？還是 LLM 只是曇花一現的隨機鸚鵡？\n姑且不論背後的語言與心智假說是否合理，\n\n以 Noam Choamsky 馬首是瞻的 形式語言學長期以來，對於句法規則的撰寫與觀察，有很好的掌握。對於抽離語意的符碼運算與系統化有高度的成就。\nLLM 的訓練與運作機制，並不是由規則導入的方式；那是什麼樣的語言觀/語言理論可以用來解釋，這些無法以「語言學方式」卻能達到目前的成就？\n\n這些都是大問題，我想嘗試開始用寫作筆記的方式幫自己整理頭緒。\n首先是 UC Berkeley 心理與腦神經科學教授 Steven t. Piantadosi 今年三月開始在LingBuzz上的一篇標題即開戰的文章：Modern language models refute Chomsky’s approach to language.\n摘要幾個重點\n\n\n\n\n\n\nCitationBibTeX citation:@online{hsieh2023,\n  author = {Hsieh, Shu-Kai},\n  title = {Notes on “{Modern} Language Models Refute {Chomsky’s}\n    Approach to Language {(Steven} {T.} {Piantadosi)}”},\n  date = {2023-10-10},\n  url = {https://loperntu.github.io/posts/2023-10-09-quarto-blogs/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nHsieh, Shu-Kai. 2023. “Notes on ‘Modern Language Models\nRefute Chomsky’s Approach to Language (Steven T.\nPiantadosi)’.” October 10, 2023. https://loperntu.github.io/posts/2023-10-09-quarto-blogs/."
  },
  {
    "objectID": "posts/2023-05-20/field_work.html",
    "href": "posts/2023-05-20/field_work.html",
    "title": "布農語田調隨行記",
    "section": "",
    "text": "等到今年終於圓夢。感謝同事麗梅老師的包容，讓我有機會一起參加台大語言所碩一必修的語言田調工作。"
  },
  {
    "objectID": "posts/2023-05-20/field_work.html#fine-tune-the-whisper-model-for-multilingual-asr",
    "href": "posts/2023-05-20/field_work.html#fine-tune-the-whisper-model-for-multilingual-asr",
    "title": "布農語田調隨行記",
    "section": "Fine-tune the Whisper model for multilingual ASR",
    "text": "Fine-tune the Whisper model for multilingual ASR\n接近尾聲，我的工作還來不及完成。隨行錄音的經驗與技術不佳，但是錄到這種實際的田調言談，混着不同腔調的中文 (還有兩位日籍同學)、布農語，甚至夾雜一些日文，讓我覺得很興奮。這是一個語音辨識的大挑戰（？）。 也許過幾天等看看 Universal Speech Model 或其他大廠的模型釋出，遷移學習是否奏效。\n語音發音儀器、言談、多模態與認知實驗 這是我覺得未來田調也可以走的方向之一，讓認知理論也能走進田調。\n大家準備回家時，每個人都收贈一本熱心的全校長翻譯的巒群布農聖經。 在回高鐵車上只因爲我開玩笑讚美了布農族司機看不出來的年紀，他就把原本要帶回家的一大袋李子給我了。一位敏感的同學分享給我，雖然這幾天收到的熱情招待很感動，但是覺得自己不值得被這樣對待，我們好像就只是在做一次性消費。的確是的，希望漢族在台的掠奪史，不要再用別的形式重現。"
  },
  {
    "objectID": "posts/2023-03-25/index.html",
    "href": "posts/2023-03-25/index.html",
    "title": "光",
    "section": "",
    "text": "雜思筆記\n首先，你是看不到光的。任何透光的東西你也看不到。 你看得到我的手，是因為它擋住光。是因為我們的視覺器官只看得到擋住光的東西。\n有光，才知道有縫隙。\n\n夫門當夜閉，閉而見月光，是有間隙也。~ 徐鍇繫傳。\n\n不過，因為有縫，才見得到光；\n\nThere is a crack in everything, that’s how the light gets in. ~ Leonard Cohen\n\n\n\n\n\nCitationBibTeX citation:@online{hsieh2023,\n  author = {Hsieh, Shu-Kai},\n  title = {光},\n  date = {2023-03-05},\n  url = {https://loperntu.github.io/posts/2023-04-23-quarto-blogs/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nHsieh, Shu-Kai. 2023. “光.” March 5, 2023. https://loperntu.github.io/posts/2023-04-23-quarto-blogs/."
  },
  {
    "objectID": "posts/2024-10-20/index.html",
    "href": "posts/2024-10-20/index.html",
    "title": "在空中相會：從佛學到大型語言模型的跨學科對話",
    "section": "",
    "text": "在這次分享中，我想先從大型語言模型如何協助佛學的語言分析開始。​ 我將以「空」為例，探討​此概念的歷時語意演變​，特別是同素異序之雙音節並列結構詞彙之間的競爭與語意轉化。(計算語言學) ​接著，我將​試圖探討佛學中的「空性」（Śūnyatā）與機器學習​表徵模型之間的相似性，​對於模型解釋與發展提供新的角度。（計算哲學）\n\n\n將佛教經典（如《心經》或《金剛經》）轉化為AI可理解的形式，並讓AI進行文本分析與語意詮釋。\n\n詞彙語意分析\n\n「空」「無」「無常」「虛無」「空性」\n「有」\n\n\nembeddings\n歷時語意演變分析\n同素異序之雙音節並列結構詞彙的競爭與語意轉化 空虛｜虛空\n\n\n\n\n空性（Śūnyatā）是佛教中非常深奧的哲學概念，主要描述所有現象的「無自性」（absense of inherent existence），即萬事萬物並無一個固定不變的本質，不是固定不變的實體存在，而是依賴於無數條件、因緣和合（Pratītyasamutpāda, interdependent origination）而存在。\n這種觀點與機器學習中的模型學習、特徵表徵（feature representation）、甚至神經網絡的權重更新過程有一些非常有趣的聯繫。\n\n「無自性」：特徵表示的流動性與條件依賴性\n\n空性強調任何事物都不是單獨、獨立存在的，而是依靠多重條件而生。這個概念可以與機器學習中「特徵表示」的學習過程聯繫起來。\n\n特徵表示的條件依賴性 在深度神經網絡中，每一層學習到的特徵表示（feature representation）是依賴於前一層輸出與網絡權重的條件而生成的。這些特徵表示（例如 CNN 中的邊緣檢測器、形狀檢測器）並非固定不變的實體，而是在不同的資料環境、不同的訓練策略下可以有不同的形式。換句話說，特徵表示的本質是無定型的，它們取決於所學習的資料、模型架構、訓練過程等條件，這正是「無自性」的體現。\n向量表示的非固定性：詞向量的空性特質 例如在自然語言處理中，詞向量（word embeddings）代表了每個詞在高維向量空間中的一個點。然而，這個向量表示是動態的，並會隨著模型、語料、甚至語境的不同而發生變化。對於某個詞（如 “bank”），它在金融領域與河流領域中具有完全不同的表示。這意味著詞向量的意涵並不是一個固定的本質，而是隨著資料的變化而變化的條件產物，正如佛教所說的「緣生性」（dependent origination）與「無自性」。\n\n\n「因緣和合」：模型結構的依賴關係與條件生成\n\n空性中的「因緣和合」觀點認為，所有現象都是眾多因緣條件的暫時組合。當條件改變時，現象的形式也隨之改變。在機器學習中，我們可以將這個觀點映射到模型的結構設計和調整過程。\n\n神經網絡層之間的條件相依性 在深度學習中，模型中的每一層（如卷積層、注意力機制層）都是在其他層的基礎上建立起來的。每一層的輸出依賴於前一層的表示和當前層的權重設定。當我們更改某一層的權重時，後續所有層的輸出也會發生變化。這就如同佛教中所說的「此有故彼有」（when this exists, that comes to be）：各層之間相互依賴，無一獨立存在，只有在特定的條件下才能顯現出當前的模式。\n生成模型中的條件生成（Conditional Generation） 例如，變分自編碼器（Variational Autoencoder, VAE）和生成對抗網絡（GAN）都是基於特定條件生成新樣本的模型。VAE 中，潛在變量（latent variables）是通過與輸入樣本的相互關係來生成新資料的，而 GAN 的生成器則依賴於與判別器（Discriminator）的對抗性條件才能生成高質量的樣本。這些生成過程反映了「因緣和合」的觀點：模型中的任何輸出都是依賴於多種條件的組合，最終形成我們看到的結果。\n\n\n「空性」：模型中的「表示空間」與潛在語意的多重流動性\n\n在機器學習中，模型學習到的知識通常表示為一個高維度的「特徵空間」或「表示空間」（representation space），這些空間並不是實體的對象，而是模式的集合。這些表示空間的變動性和抽象性揭示了「空性」的特質。\n\n表示空間中的空性：自編碼器與表示壓縮 在自編碼器（Autoencoder）模型中，原始資料被壓縮到潛在空間（latent space）中。這個潛在空間中的每一個點雖然代表了原始資料的一個抽象表示，但它本身並不具有固定的形態或意涵。它只是某個條件集合的符號化表徵，隨著模型訓練過程的變化而改變。因此，潛在空間中的表示是一種「空的表示」：它僅僅是可能性的表現，而不是某個具體物體的再現。\n生成模型的空性與虛擬現象的顯現 生成模型（如 GAN）在創造圖像、語音或文本時，它所生成的現象（如一張逼真的人臉）在數學意義上只是一組向量的變換結果，並非真實世界中存在的對象。這樣的生成圖像是一種「如幻如夢」的存在形式：它有形象但無實體，有模式但無自性。這正是佛教空性思想中的「幻象觀」——所有現象都是在特定條件下暫時顯現的虛幻（illusory）存在。\n\n\n模型泛化（Generalization）與「無常性」的理解\n\n空性強調現象的變動性（impermanence），認為一切事物都不斷處於變化之中。這在機器學習中對應於模型的泛化能力。當模型執著於某一種模式或資料集時，它的表現就會在新資料集上出現偏差。\n\n過擬合與執著：模型的無常性表現 當一個模型過度擬合某個訓練集時，它便對該資料集的特定模式產生了「執著」，忽視了資料背後的空性與變動性。一旦進入新的資料分佈（distribution shift），模型的表現會崩潰。這反映了佛教中對執著的批判：當我們過於執著於現象的固定形式時，便無法適應新的環境與條件。因此，設計更具泛化能力的模型（如正則化、遷移學習）可以被視為一種「破執」（overcoming clinging），使模型能更靈活地看待資料分佈的無常性。\n\n\n空性的實踐：從複雜模型到簡化模型的轉化\n\n在實際應用中，模型的複雜性常常導致其對特定條件的過度依賴。通過模型壓縮（model compression）和知識蒸餾（knowledge distillation），我們可以將大型模型的知識轉移到較小的模型中，去除不必要的依賴與冗餘。\n\n模型壓縮中的空性實踐 這種壓縮過程可以被理解為「去除多餘的執著」，即找到模型中真正有效的表示，而不是所有細節的重現。透過這種去繁從簡的方式，我們達到了一種更接近「空性」的模型結構：模型中的每個參數和特徵都是在去除了冗餘條件後的最小表現形式。\n\n總結來說，佛教的空性觀點可以為機器學習模型提供新的思考視角。"
  },
  {
    "objectID": "posts/2024-10-20/index.html#語言與觀念的轉化文字般若的當代應用",
    "href": "posts/2024-10-20/index.html#語言與觀念的轉化文字般若的當代應用",
    "title": "在空中相會：從佛學到大型語言模型的跨學科對話",
    "section": "",
    "text": "將佛教經典（如《心經》或《金剛經》）轉化為AI可理解的形式，並讓AI進行文本分析與語意詮釋。\n\n詞彙語意分析\n\n「空」「無」「無常」「虛無」「空性」\n「有」\n\n\nembeddings\n歷時語意演變分析\n同素異序之雙音節並列結構詞彙的競爭與語意轉化 空虛｜虛空"
  },
  {
    "objectID": "posts/2024-10-20/index.html#空性śūnyatā與機器學習模型",
    "href": "posts/2024-10-20/index.html#空性śūnyatā與機器學習模型",
    "title": "在空中相會：從佛學到大型語言模型的跨學科對話",
    "section": "",
    "text": "空性（Śūnyatā）是佛教中非常深奧的哲學概念，主要描述所有現象的「無自性」（absense of inherent existence），即萬事萬物並無一個固定不變的本質，不是固定不變的實體存在，而是依賴於無數條件、因緣和合（Pratītyasamutpāda, interdependent origination）而存在。\n這種觀點與機器學習中的模型學習、特徵表徵（feature representation）、甚至神經網絡的權重更新過程有一些非常有趣的聯繫。\n\n「無自性」：特徵表示的流動性與條件依賴性\n\n空性強調任何事物都不是單獨、獨立存在的，而是依靠多重條件而生。這個概念可以與機器學習中「特徵表示」的學習過程聯繫起來。\n\n特徵表示的條件依賴性 在深度神經網絡中，每一層學習到的特徵表示（feature representation）是依賴於前一層輸出與網絡權重的條件而生成的。這些特徵表示（例如 CNN 中的邊緣檢測器、形狀檢測器）並非固定不變的實體，而是在不同的資料環境、不同的訓練策略下可以有不同的形式。換句話說，特徵表示的本質是無定型的，它們取決於所學習的資料、模型架構、訓練過程等條件，這正是「無自性」的體現。\n向量表示的非固定性：詞向量的空性特質 例如在自然語言處理中，詞向量（word embeddings）代表了每個詞在高維向量空間中的一個點。然而，這個向量表示是動態的，並會隨著模型、語料、甚至語境的不同而發生變化。對於某個詞（如 “bank”），它在金融領域與河流領域中具有完全不同的表示。這意味著詞向量的意涵並不是一個固定的本質，而是隨著資料的變化而變化的條件產物，正如佛教所說的「緣生性」（dependent origination）與「無自性」。\n\n\n「因緣和合」：模型結構的依賴關係與條件生成\n\n空性中的「因緣和合」觀點認為，所有現象都是眾多因緣條件的暫時組合。當條件改變時，現象的形式也隨之改變。在機器學習中，我們可以將這個觀點映射到模型的結構設計和調整過程。\n\n神經網絡層之間的條件相依性 在深度學習中，模型中的每一層（如卷積層、注意力機制層）都是在其他層的基礎上建立起來的。每一層的輸出依賴於前一層的表示和當前層的權重設定。當我們更改某一層的權重時，後續所有層的輸出也會發生變化。這就如同佛教中所說的「此有故彼有」（when this exists, that comes to be）：各層之間相互依賴，無一獨立存在，只有在特定的條件下才能顯現出當前的模式。\n生成模型中的條件生成（Conditional Generation） 例如，變分自編碼器（Variational Autoencoder, VAE）和生成對抗網絡（GAN）都是基於特定條件生成新樣本的模型。VAE 中，潛在變量（latent variables）是通過與輸入樣本的相互關係來生成新資料的，而 GAN 的生成器則依賴於與判別器（Discriminator）的對抗性條件才能生成高質量的樣本。這些生成過程反映了「因緣和合」的觀點：模型中的任何輸出都是依賴於多種條件的組合，最終形成我們看到的結果。\n\n\n「空性」：模型中的「表示空間」與潛在語意的多重流動性\n\n在機器學習中，模型學習到的知識通常表示為一個高維度的「特徵空間」或「表示空間」（representation space），這些空間並不是實體的對象，而是模式的集合。這些表示空間的變動性和抽象性揭示了「空性」的特質。\n\n表示空間中的空性：自編碼器與表示壓縮 在自編碼器（Autoencoder）模型中，原始資料被壓縮到潛在空間（latent space）中。這個潛在空間中的每一個點雖然代表了原始資料的一個抽象表示，但它本身並不具有固定的形態或意涵。它只是某個條件集合的符號化表徵，隨著模型訓練過程的變化而改變。因此，潛在空間中的表示是一種「空的表示」：它僅僅是可能性的表現，而不是某個具體物體的再現。\n生成模型的空性與虛擬現象的顯現 生成模型（如 GAN）在創造圖像、語音或文本時，它所生成的現象（如一張逼真的人臉）在數學意義上只是一組向量的變換結果，並非真實世界中存在的對象。這樣的生成圖像是一種「如幻如夢」的存在形式：它有形象但無實體，有模式但無自性。這正是佛教空性思想中的「幻象觀」——所有現象都是在特定條件下暫時顯現的虛幻（illusory）存在。\n\n\n模型泛化（Generalization）與「無常性」的理解\n\n空性強調現象的變動性（impermanence），認為一切事物都不斷處於變化之中。這在機器學習中對應於模型的泛化能力。當模型執著於某一種模式或資料集時，它的表現就會在新資料集上出現偏差。\n\n過擬合與執著：模型的無常性表現 當一個模型過度擬合某個訓練集時，它便對該資料集的特定模式產生了「執著」，忽視了資料背後的空性與變動性。一旦進入新的資料分佈（distribution shift），模型的表現會崩潰。這反映了佛教中對執著的批判：當我們過於執著於現象的固定形式時，便無法適應新的環境與條件。因此，設計更具泛化能力的模型（如正則化、遷移學習）可以被視為一種「破執」（overcoming clinging），使模型能更靈活地看待資料分佈的無常性。\n\n\n空性的實踐：從複雜模型到簡化模型的轉化\n\n在實際應用中，模型的複雜性常常導致其對特定條件的過度依賴。通過模型壓縮（model compression）和知識蒸餾（knowledge distillation），我們可以將大型模型的知識轉移到較小的模型中，去除不必要的依賴與冗餘。\n\n模型壓縮中的空性實踐 這種壓縮過程可以被理解為「去除多餘的執著」，即找到模型中真正有效的表示，而不是所有細節的重現。透過這種去繁從簡的方式，我們達到了一種更接近「空性」的模型結構：模型中的每個參數和特徵都是在去除了冗餘條件後的最小表現形式。\n\n總結來說，佛教的空性觀點可以為機器學習模型提供新的思考視角。"
  },
  {
    "objectID": "posts/2024-11-02/index.html",
    "href": "posts/2024-11-02/index.html",
    "title": "量子語言學筆記：波粒二象的啟示",
    "section": "",
    "text": "波粒二象性可能可以提供一種視角來理解語言中「語意（meaning）」與「形式（form）」之間的轉換關係。波的連續性代表語意的流動性和多義性，而一旦語意被「使用」或「表達」出來，它便會透過語言形式具象化，類似於波的坍縮成粒子，成為一個個具體的詞語或句子。\n從物理學的角度來看，這與量子力學中「測量」帶來的波函數坍縮概念有相似之處。當語意處於未被明確表達的狀態時，它像波一樣具有連續的潛在性（continuous potentiality）；而當我們將它「測量」或表達出來時，它便轉化為離散的語言單位。這樣的比喻不僅可以用於理解語言的雙重特性，也可能提供探索語言如何在意識中被感知和使用的一種新視角。"
  },
  {
    "objectID": "posts/2024-11-02/index.html#形式與意義的關係",
    "href": "posts/2024-11-02/index.html#形式與意義的關係",
    "title": "量子語言學筆記：波粒二象的啟示",
    "section": "",
    "text": "波粒二象性可能可以提供一種視角來理解語言中「語意（meaning）」與「形式（form）」之間的轉換關係。波的連續性代表語意的流動性和多義性，而一旦語意被「使用」或「表達」出來，它便會透過語言形式具象化，類似於波的坍縮成粒子，成為一個個具體的詞語或句子。\n從物理學的角度來看，這與量子力學中「測量」帶來的波函數坍縮概念有相似之處。當語意處於未被明確表達的狀態時，它像波一樣具有連續的潛在性（continuous potentiality）；而當我們將它「測量」或表達出來時，它便轉化為離散的語言單位。這樣的比喻不僅可以用於理解語言的雙重特性，也可能提供探索語言如何在意識中被感知和使用的一種新視角。"
  },
  {
    "objectID": "posts/2025-01-13/index.html#一切都是-lagrangian-子流形",
    "href": "posts/2025-01-13/index.html#一切都是-lagrangian-子流形",
    "title": "量子語言學筆記：Lagrangian submanifold？",
    "section": "一切都是 Lagrangian 子流形？",
    "text": "一切都是 Lagrangian 子流形？\n忘記粒子，也忘記波動吧。Alan Weinstein 開玩笑地說，宇宙並不是由粒子或波組成，而是由完全不同的東西構建的：Lagrangian 子流形（Lagrangian submanifolds）。那麼，什麼是 Lagrangian 子流形？為什麼值得關心？要理解這個概念，需要先了解什麼是相空間（phase space）。"
  },
  {
    "objectID": "posts/2025-01-13/index.html#相空間不是狀態空間",
    "href": "posts/2025-01-13/index.html#相空間不是狀態空間",
    "title": "量子語言學筆記：Lagrangian submanifold？",
    "section": "相空間：不是「狀態空間」",
    "text": "相空間：不是「狀態空間」\n通常，人們提到「相空間」時，其實說的是「狀態空間」（state space），這其實是不正確的。因此，我們先正確定義相空間：相空間是一種抽象空間（注意，不是時空），每個點代表一個粒子的狀態，由位置 \\(q\\) 和動量 \\(p\\) 描述。\n\n想像有一個世界叫做「辛幾何世界」(symplectic)，它跟一般的平面或空間不一樣，這個世界裡的每個點都同時代表兩件事情：一個位置（像座標 \\(q\\)) 和它的變化速度（像動量 \\(𝑝\\))。所以，這個世界就像是物理課上的「相空間」，用來描述東西在哪裡和怎麼動。\n\n目前，我們只是有了一個「沒有幾何結構的空間」。你可以給它附加某種度量，讓它變成黎曼空間（Riemannian space），或者給它一種不同的結構，讓它變成辛幾何空間（symplectic space）。\n我們選擇後者，這樣會給相空間賦予一種特殊的結構：一個稱為辛形式（symplectic form）的二形式，通常記為 \\(\\omega\\)。這個 \\(\\omega\\) 允許我們在相空間中定義「面積」，並且（更重要的是）規定了系統如何隨時間演化。這個規則非常簡潔優雅：\n\\[ \\{f, H\\} = \\frac{df}{dt} \\]\n\n你可以把辛幾何世界想像成一個大的溜冰場，滑冰的每個人都有兩個方向：往前滑（位置）和轉圈圈（動量）。而 Lagrangian 子流形就像是一條特別的滑冰路線： 它很聰明，只需要控制一部分動作（比如只專注於直線滑，或者只專注於轉圈圈）。 而且在這條路線上，滑冰的人之間不會互相干擾，大家的動作都協調得非常好。"
  },
  {
    "objectID": "posts/2025-01-13/index.html#辛幾何與哈密頓動力學",
    "href": "posts/2025-01-13/index.html#辛幾何與哈密頓動力學",
    "title": "量子語言學筆記：Lagrangian submanifold？",
    "section": "辛幾何與哈密頓動力學",
    "text": "辛幾何與哈密頓動力學\n\n這裡 \\(H\\) 是哈密頓量（Hamiltonian，描述系統的能量），而 \\(\\{f, H\\}\\) 是泊松括號（Poisson bracket）。泊松括號使用 ( ) 測量函數 \\(f\\) 沿著由 \\(H\\) 產生的流的變化。\n如果 \\(\\{f, H\\} = 0\\)，那麼 \\(f\\) 是一個守恆量。例如，時間 \\(t\\) 必須滿足 \\(\\{t, H\\} = 1\\)。這意味著時間必須以自己為基準線性演化——這是一個非常美妙的條件。"
  },
  {
    "objectID": "posts/2025-01-13/index.html#那麼lagrangian-子流形是什麼",
    "href": "posts/2025-01-13/index.html#那麼lagrangian-子流形是什麼",
    "title": "量子語言學筆記：Lagrangian submanifold？",
    "section": "那麼，Lagrangian 子流形是什麼？",
    "text": "那麼，Lagrangian 子流形是什麼？\nLagrangian 子流形 \\(L\\) 是相空間中一個特殊的子空間，滿足以下條件：\n\n它是辛形式 \\(\\omega\\) 的核（null space），也就是說，\\(\\omega\\) 在這個子空間上完全消失。\n用簡單的話說，在這個子空間上，\\(\\omega\\) 測量的所有「面積」都是零。\n\n在物理上，Lagrangian 子流形通常對應於某些具有物理意義的狀態集合，例如一個系統的可能狀態。Lagrangian 子流形幫助我們找到一個系統的平衡點或特殊運動方式。比如： 如果你丟一個球，它的路徑可以看成是一條 Lagrangian 子流形；在一些更複雜的世界裡，比如研究量子力學或宇宙的運動，Lagrangian 子流形也能幫助我們理解那些看起來很複雜的規律。"
  },
  {
    "objectID": "posts/2025-01-13/index.html#物理中的應用從約束到量子化",
    "href": "posts/2025-01-13/index.html#物理中的應用從約束到量子化",
    "title": "量子語言學筆記：Lagrangian submanifold？",
    "section": "物理中的應用：從約束到量子化",
    "text": "物理中的應用：從約束到量子化\n\n1. 約束系統\n大多數情況下，物理系統（甚至現實生活中的系統）都會有某些約束。例如，擺錘的運動受到固定長度的限制。在這種情況下，系統的可能狀態可能位於某個 Lagrangian 子流形上。許多物理約束都可以用這種幾何語言來表達。\n\n\n2. 量子化\n從經典物理到量子物理的過渡（即量子化）通常是一個難以精確定義的過程。但即便如此，量子化的某些條件也可以用 Lagrangian 子流形來描述。特定的「量子化條件」選出了一些特定的 Lagrangian 子流形，對應於允許的量子態。"
  },
  {
    "objectID": "posts/2025-01-13/index.html#為什麼-weinstein-說一切都是-lagrangian-子流形",
    "href": "posts/2025-01-13/index.html#為什麼-weinstein-說一切都是-lagrangian-子流形",
    "title": "量子語言學筆記：Lagrangian submanifold？",
    "section": "為什麼 Weinstein 說「一切都是 Lagrangian 子流形」？",
    "text": "為什麼 Weinstein 說「一切都是 Lagrangian 子流形」？\nWeinstein 的這個說法背後有深刻的意義：它主張現實世界的基本構成物不是粒子，也不是波動，而是位於相空間中的這些特殊子空間——Lagrangian 子流形。這些子流形編碼了系統的動力學、約束，甚至其量子性質。\n一些人認為，這表明科學從「物體」的觀點轉向了「關係」的觀點（特別是範疇論研究者）。例如：\n\n位置與動量之間的關係，\n能量與時間之間的關係，\n\n這些關係都可以用辛幾何語言來描述，而這些描述往往集中於 Lagrangian 子流形之中。"
  },
  {
    "objectID": "posts/2025-01-13/index.html#補充說明",
    "href": "posts/2025-01-13/index.html#補充說明",
    "title": "量子語言學筆記：Lagrangian submanifold？",
    "section": "補充說明",
    "text": "補充說明\n\n相空間與辛幾何提供了一種統一的視角，能夠連結經典物理與量子物理。\nLagrangian 子流形則是這種視角中的關鍵元素，幫助我們理解一個系統的所有可能狀態及其物理意義。\n\n這段話最終告訴我們：Lagrangian 子流形不僅僅是一個數學概念，它還是一種新的思考現實的方法。"
  },
  {
    "objectID": "posts/2023-03-05/index.html",
    "href": "posts/2023-03-05/index.html",
    "title": "量子語言學筆記",
    "section": "",
    "text": "Language as Form-Meaning Pairing Emerges Through Usage\n\nLanguage inherently pairs form with meaning, and this pairing emerges through usage in communication.\n語言本質上是形式與意義的配對，而這種配對是在溝通使用中逐漸突現的。\n\nLanguage and Meaning are Embodied\n\nThe construction and understanding of language and meaning are deeply rooted in bodily experiences.\n語言與意義的構建與理解深深根植於身體經驗之中。\n\nThe Structural Form of Language as a Stable By-Product\n\nThe structural form of language is a relatively stable by-product of communication, variation, and interaction processes.\n語言的結構形式是在交際溝通與變異過程中相對穩定的副產品。\n\nComputational Representation of Meaning as Tensors\n\nSemantic computations and representations in language can be modeled using tensors.\n語意的計算與表徵可以通過張量來建模。\n\nConsciousness and Cognition as Quantum Phenomena\n\nConsciousness, as well as cognitive processes, may be understood as quantum phenomena.\n意識與認知過程或可理解為量子現象。"
  },
  {
    "objectID": "posts/2023-03-05/index.html#tenets",
    "href": "posts/2023-03-05/index.html#tenets",
    "title": "量子語言學筆記",
    "section": "",
    "text": "Language as Form-Meaning Pairing Emerges Through Usage\n\nLanguage inherently pairs form with meaning, and this pairing emerges through usage in communication.\n語言本質上是形式與意義的配對，而這種配對是在溝通使用中逐漸突現的。\n\nLanguage and Meaning are Embodied\n\nThe construction and understanding of language and meaning are deeply rooted in bodily experiences.\n語言與意義的構建與理解深深根植於身體經驗之中。\n\nThe Structural Form of Language as a Stable By-Product\n\nThe structural form of language is a relatively stable by-product of communication, variation, and interaction processes.\n語言的結構形式是在交際溝通與變異過程中相對穩定的副產品。\n\nComputational Representation of Meaning as Tensors\n\nSemantic computations and representations in language can be modeled using tensors.\n語意的計算與表徵可以通過張量來建模。\n\nConsciousness and Cognition as Quantum Phenomena\n\nConsciousness, as well as cognitive processes, may be understood as quantum phenomena.\n意識與認知過程或可理解為量子現象。"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This website documents my learning journey."
  }
]