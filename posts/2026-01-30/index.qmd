---
title: "人文 AI"
description: "文學院的未來？"
author:
  - name: Shu-Kai Hsieh
    url: https://loperntu.github.io/
    orcid: 0000-0001-9674-1249
    affiliation: 台大語言學研究所 LOPE Lab
    affiliation-url: https://lope.github.io/ 
date: 01-30-2026
categories: [Humanities and AI] # 可以填入分類，例如：[Quantum Semantics]
citation: 
  url: https://loperntu.github.io/posts/2026-01-30/ 
image: preview_image.jpg
draft: false # 設為 true 時，文章不會出現在列表頁，直到你準備好發布
---


這是個讓人文領域焦慮的時代。但是除了焦慮，與過度吹捧人文價值的另一端，有沒有更深刻的討論？

> LLMs are not models of human cognition, but **geometric mirrors of human language**: they reflect what survives statistical compression, and distort what depends on history, embodiment, and culture.

從整個 LLM 時代的地基問題開始：

- 語言與世界的關係：不是【指稱】，而是【投影】。

傳統想法是，語言指向世界（reference）；但這在 LLM／語言幾何學視角下，更準確的說法是：語言是人類與世界互動後，在認知—文化—行動層面形成的「壓縮投影」（projection）。

但世界不是直接進入語言其中。這個鏈【世界 --> 感知／行動 --> 社會互動 --> 慣例 --> 語言】說明了，語言本身已經是二手甚至三手資料。所以簡要地說，語言不是世界的鏡子，而是世界在人類可溝通尺度上的低維投影。LLM學到的，其實是：「人類如何談論世界」的幾何，而不是世界本身。這一點作為背景哲學預設，非常重要。

- 那 LLM 的問題到底是什麼？

在學界與業界常被說成一句話：「LLM 缺乏 grounding」。但這句話其實太簡略了。
也許更精確的說法是，LLM 的向量空間，只有語言與語言之間的閉合動力系統，缺乏「語言—行動—後果」的回饋環。

所以它的極限並不是不知道世界長什麼樣，而是不知道「做錯會怎樣」。這也是為什麼它能流暢描述危險，卻無法自然形成風險感、責任感、後果權重。

- 那當前熱門的物理與具身 AI( Physical / Embodied AI) 能完整補這一塊嗎？

也許部分能，但可能不是我們想像的那種「解決」。更精確的說法是，Physical AI 補的是因果回饋，不是語義真理。

具身 AI 能提供因果回饋行動到結果；試錯到代價；時間連續性等等。
這對 LLM 來說的確是質變。但是，它補的是因果回饋 causal grounding，不是語義真理 semantic grounding。它讓模型知道：「這樣說／這樣做，世界會反彈」，但不會自動產生**價值**、**意義**、**規範**。


所以，我目前的想法是，Physical AI 也許無法解決語言出發的 LLM paradigm，只能讓這個 paradigm 不再那麼自洽。

為什麼？因為語言是跨主體、跨世代、跨文化的； 而身體經驗是局部、當下、個體化的。
一個機器人的身體經驗，不等於人類的身體史，更無法等於文明的集體經驗。
Physical AI 會造成的是：某些向量方向開始有實際代價；但整體語言拓撲不會因此崩解。


> Language does not ground in the world directly.
It grounds in socially stabilized action.
Physical AI adds consequences to actions,
but **meaning still lives in the linguistic geometry shaped by culture and history.**


如果一個系統只有語言幾何，卻沒有身體與歷史，那它所顯現的「語言結構」，
是否正好讓我們看見，有哪些語言現象其實從來就不需要世界？

**從這個角度看，人文學科的位置反而更穩固。**



但，這些討論對於教育決策者太抽象了，我們需要的說詞，比較像是以下這些。

---

## AI 時代的「關鍵能力」其實是人文能力

**例子 1｜誰能問對問題？**

* 同一個 AI，一般學生問：「幫我摘要這篇文章。」
* 受過文史哲訓練的學生應該會要問：「這段論證的前提是什麼？它隱含了哪個時代的價值觀？如果換成東亞語境，結論還成立嗎？」

**差異不在 AI，而在提問者的詮釋框架。**
這正是文學院應該訓練的能力：**概念史、脈絡化、問題重構**。

---

## 修正 AI 偏見，本質是「人文判斷」

**例子 2｜AI 的偏見不是 bug，是文化產物**

* AI 在法律、性別、族群、歷史敘事上的偏誤，往往來自：
  * 英語中心的語料
  * 西方近代自由主義的價值預設

* 能「看出這種問題」的人，不是期待最會寫程式的，而是**懂得比較文明、歷史、思想傳統的人**。


文學院可以直接產出：
**「AI 偏見診斷模組」**（Bias Audit for LLMs）
由哲學、歷史、語言學共同設計測試題，而非只靠 accuracy。

---

## 鑑定 AI 產出好壞，是「高階閱讀能力」

**例子 3｜AI 很會寫，但不一定「對」**

* AI 可以生成看似完美的歷史解釋，但可能涉及：

  * 時代錯置（anachronism）
  * 概念混用（把現代概念硬套古代文本）
  * 修辭流暢但論證空洞

**能辨識這些問題的，是訓練過精讀、詮釋、文本批判的人。**
這是文學院學生每天在做、但過去「無法量化」的能力，也是文學院應該訓練的能力。

---

## 從「會用 AI」到「駕馭 AI」

**例子 4｜多輪對話不是聊天，是論證工程**

* 與 AI 的高品質互動其實是：

  1. 設定問題邊界
  2. 拆解假設
  3. 逐步修正
  4. 要求反證與替代解釋

這正是涉及哲學的辯證訓練、歷史的反事實思考、文學的多重視角解讀。



如果學生不懂歷史與概念史，他們無法發現 AI 把現代價值硬套進古代文本；
如果沒有哲學訓練，他們無法拆解 AI 論證中的隱含前提；
如果沒有語言與敘事敏感度，他們只會被「寫得很流暢」的錯誤答案帶著走。

俏皮一點說：文學院是在教「如何成為 AI 的導演，而不是打字員」；如果 AI 是放大器，文學院確保我們放大的不是偏見與錯誤。

---

## 給教育決策者們的「投資語言」

未來教育的關鍵不在答案，而在三件事：
問對問題、修正偏見、鑑定品質。這三件事，正是人文教育最成熟、卻長期被低估的能力。

> 「AI 已經會給答案，但未來社會最缺的，是能判斷哪些答案值得相信、哪些問題值得問的人。
> 文學院不是 AI 時代的成本中心，而是 **整個大學 AI 素養的控制塔**。」
> 文學院也不是 AI 時代的防守單位，而是與 AI 共舞的判斷力的中樞投資。



## QA

### Q1｜「這不就是在教 prompt engineering 嗎？為什麼一定要文學院？」

**短答版**
不是。Prompt engineering 是讓 AI 更快給答案，我們做的是讓學生知道什麼時候不該相信答案。

**補強版**

- Prompt 是操作層，人文訓練是判斷層
* AI 偏見、時代錯置、價值預設，無法靠技巧修正
* 文學院訓練的是：概念史、詮釋、反證與多重視角。我們教的是「如何拆解 AI 的論證」，不是如何取悅 AI。